{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name:Aughdon Breslin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whole column is full of NaN\n",
    "data['Unnamed: 32'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "del data['id']\n",
    "del data['Unnamed: 32']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         -1        17.99         10.38          122.80     1001.0   \n",
       "1         -1        20.57         17.77          132.90     1326.0   \n",
       "2         -1        19.69         21.25          130.00     1203.0   \n",
       "3         -1        11.42         20.38           77.58      386.1   \n",
       "4         -1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
    "data['diagnosis'] = data['diagnosis'].replace(['B','M'],[1,-1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30) (455,) (114,)\n"
     ]
    }
   ],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
    "X = data.values[:, 1:]\n",
    "y = data.values[:, 0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 30\n",
      "test mean = \n",
      "[ 0.02090745  0.16431943  0.03098111  0.01492717 -0.01921133  0.10117201\n",
      "  0.08443932  0.02966041  0.04062134  0.09494587  0.04653776  0.06428674\n",
      "  0.10007498  0.00538732  0.06601984  0.11839521  0.0676146   0.1268942\n",
      "  0.01037956  0.16193656  0.01337734  0.1134187   0.03390329 -0.00517026\n",
      " -0.02001553  0.07842781  0.11756424  0.01212816 -0.02700714  0.12744666]\n",
      "test std = \n",
      "[0.98484027 1.15221324 0.99783436 0.91061412 1.0932457  1.2099472\n",
      " 1.1161288  1.0269985  1.00338544 1.16387855 0.86929628 1.0858163\n",
      " 0.95737985 0.73797557 1.25342805 1.10484161 0.84117872 1.20913453\n",
      " 1.04276269 1.05856313 0.94112163 1.03290181 0.96071251 0.84629366\n",
      " 1.04566915 1.12995399 1.20449902 1.03206998 0.90827971 1.06240744]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "n,d = x_train.shape\n",
    "print(n,d)\n",
    "mu = np.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = np.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(np.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(np.std(x_test, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    total = 0\n",
    "    #     sum 1 to n\n",
    "    for i in range(len(y)):\n",
    "        #             ln(1+e^(-(-1 or 1)*(1-by-d)*(d-by-1)))\n",
    "        total += np.log(1+np.exp(-y[i]*np.matmul(np.transpose(x[i]),w)))\n",
    "    # 1/n*\n",
    "    total /= len(y)\n",
    "    # + lambda/2*||w||_2^2\n",
    "    total += (lam/2)*(np.linalg.norm(w,2)**2)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    gradAtW = 0\n",
    "    # sum i to n\n",
    "    for i in range(len(y)):\n",
    "        #          y[i]x[i]/(1+exp(y[i](x[i]^T)w)\n",
    "        gradAtW += y[i]*np.array(x[i])/(1+np.exp(y[i]*np.matmul(np.transpose(x[i]),w)))\n",
    "    # -1/n*\n",
    "    gradAtW /= -len(y)\n",
    "    # + lambda*w\n",
    "    gradAtW += lam*np.array(w)\n",
    "    return gradAtW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 2, n = 3\n",
    "# w = [1,2]\n",
    "# x = [[1,2],\n",
    "#     [3,4],\n",
    "#     [5,6]]\n",
    "# y = [-1,1,-1]\n",
    "# gradient(w,x,y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    objvals = []\n",
    "    for i in range(max_epoch):\n",
    "        grad = gradient(w,x,y,lam)\n",
    "        w = w - learning_rate*grad\n",
    "        objvals.append(objective(w, x, y, lam))\n",
    "    return w, objvals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.14557392 -0.08553148 -0.14815275 -0.13892117 -0.06118345 -0.1126143\n",
      " -0.13519603 -0.16133742 -0.05421138  0.04497547 -0.11132205  0.02507147\n",
      " -0.10538446 -0.09953842  0.04794852 -0.0211182   0.00038428 -0.05111398\n",
      "  0.02964078  0.032614   -0.16088395 -0.09986907 -0.16102357 -0.14765086\n",
      " -0.08472199 -0.11511386 -0.12817047 -0.16687846 -0.08380478 -0.04780792]\n",
      "\n",
      "Objective Values: [0.9231988025677454, 0.8902827502124717, 0.859102109693234, 0.8296301404601011, 0.8018296159510284, 0.7756530289275281, 0.7510432194579952, 0.7279344653483205, 0.7062540110741883, 0.6859239336944855, 0.6668631796657779, 0.6489895798196352, 0.6322216692941516, 0.6164801932388766, 0.6016892451507545, 0.587777042340636, 0.5746763815196243, 0.5623248358339419, 0.5506647575836952, 0.5396431445432044, 0.5292114174492809, 0.5193251452238982, 0.5099437446218023, 0.5010301729164941, 0.4925506260171913, 0.4844742498188584, 0.47677286931209867, 0.4694207377110809, 0.46239430633295053, 0.45567201497280274, 0.44923410190607305, 0.44306243229855463, 0.4371403436289537, 0.4314525066701951, 0.4259848005916635, 0.4207242008062981, 0.41565867827420033, 0.4107771090750224, 0.4060691931661256, 0.40152538134696925, 0.397136809548936, 0.392895239662066, 0.38879300619495905, 0.3848229681411788, 0.38097846549500936, 0.37725327992166185, 0.37364159914259365, 0.3701379846459785, 0.3667373423761688, 0.3634348960947812, 0.36022616314033157, 0.35710693234366075, 0.3540732438831882, 0.35112137088768386, 0.3482478026151639, 0.34544922905501607, 0.34272252681677856, 0.34006474618351595, 0.3374730992205009, 0.3349449488413162, 0.3324777987435502, 0.3300692841352207, 0.3277171631810206, 0.3254193091045709, 0.3231737028891594, 0.3209784265251028, 0.31883165675686226, 0.31673165928755687, 0.3146767834025363, 0.31266545697725967, 0.31069618183797737, 0.3087675294465873, 0.3068781368836751, 0.30502670310604274, 0.30321198545719363, 0.30143279641110526, 0.2996880005313549, 0.29797651162921884, 0.29629729010575157, 0.2946493404641416, 0.29303170897977737, 0.2914434815165113, 0.2898837814785413, 0.2883517678882165, 0.2868466335808301, 0.2853676035081946, 0.28391393314344, 0.2824849069800596, 0.28107983711878726, 0.2796980619363669, 0.2783389448307397, 0.2770018730375795, 0.27568625651349665, 0.2743915268815682, 0.2731171364351771, 0.2718625571964326, 0.27062728002571934, 0.2694108137791628, 0.26821268451102875, 0.267032434718291]\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w = [1/len(x_train[0])] * len(x_train[0])\n",
    "optWeightsG, objListG = gradient_descent(x_train, y_train, 0, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsG)\n",
    "print()\n",
    "print(\"Objective Values:\", objListG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.10256293 -0.06216911 -0.10395894 -0.09767916 -0.04526642 -0.07787667\n",
      " -0.09262274 -0.1102918  -0.04053012  0.02414954 -0.07748494  0.01055564\n",
      " -0.07361676 -0.07044103  0.02601952 -0.01838699 -0.00529712 -0.03847013\n",
      "  0.01366675  0.01626286 -0.11209119 -0.07166607 -0.11195825 -0.10300223\n",
      " -0.06118114 -0.08081467 -0.08925162 -0.11480412 -0.06135565 -0.03679574]\n",
      "\n",
      "Objective Values: [0.9337031828273429, 0.8957335374096531, 0.8605131554716192, 0.8279202348780442, 0.7978234145601023, 0.7700832782221895, 0.7445543653944723, 0.7210876172111748, 0.6995330889067501, 0.6797426889337838, 0.6615726894637477, 0.6448858021150483, 0.6295527035888147, 0.61545299289606, 0.6024756367717088, 0.5905190014700603, 0.5794905807165575, 0.5693065211985947, 0.5598910288779736, 0.551175718995031, 0.5430989540374956, 0.535605198841682, 0.5286444106073451, 0.5221714734864235, 0.5161456818768131, 0.510530272955674, 0.5052920067683588, 0.50040079092014, 0.4958293462828356, 0.49155290990329326, 0.4875489713304332, 0.48379703875741936, 0.4802784316362783, 0.4769760967190941, 0.4738744447848228, 0.4709592056068149, 0.468217298993924, 0.46563671999282275, 0.46320643656916677, 0.46091629829062797, 0.4587569547168518, 0.456719782361682, 0.45479681923373255, 0.4529807060844613, 0.4512646336004493, 0.4496422948703607, 0.44810784253884184, 0.4466558501308643, 0.4452812770921717, 0.4439794371456427, 0.44274596961067714, 0.44157681337398685, 0.44046818323628206, 0.4394165483909102, 0.43841861281817923, 0.4374712974033528, 0.43657172360763347, 0.4357171985401556, 0.4349052012955282, 0.434133370436001, 0.43339949251015863, 0.43270149151140147, 0.43203741918950195, 0.43140544613744164, 0.4308038535836125, 0.43023102582650136, 0.42968544325520425, 0.42916567590470095, 0.42867037749975856, 0.4281982799457765, 0.42774818822885846, 0.42731897569092503, 0.426909579648871, 0.42651899732963755, 0.4261462820956081, 0.4257905399370676, 0.4254509262105344, 0.4251266426036395, 0.4248169343089364, 0.42452108739052774, 0.42423842632880515, 0.4239683117298153, 0.4237101381869379, 0.42346333228354793, 0.4232273507263069, 0.4230016785995448, 0.4227858277319863, 0.4225793351677693, 0.42238176173434855, 0.42219269070045023, 0.42201172651780594, 0.42183849364083287, 0.4216726354189312, 0.42151381305642927, 0.4213617046356054, 0.4212160041985599, 0.42107642088400377, 0.4209426781153514, 0.42081451283673443, 0.4206916747938298]\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w = [1/len(x_train[0])] * len(x_train[0])\n",
    "optWeightsRG, objListRG = gradient_descent(x_train, y_train, 1, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsRG)\n",
    "print()\n",
    "print(\"Objective Values:\", objListRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveSGD(w, xi, yi, lam):\n",
    "#     return np.log(1+np.exp(-yi*np.matmul(np.transpose(xi),w))) + lam/2*(np.linalg.norm(w,2)**2)\n",
    "# def gradientSGD(w, xi, yi, lam):\n",
    "#     return -yi*np.array(xi)/(1+np.exp(yi*np.matmul(np.transpose(xi),w))) + lam*np.array(w)\n",
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    # log(1+exp(-yi*(xi^T)*w)) + lambda/2*w**2\n",
    "    obj = np.log(1+np.exp(-yi*np.matmul(np.transpose(xi),w))) + lam/2*(np.linalg.norm(w,2)**2)\n",
    "    # -yi*xi/(1+exp(yi*(xi^T)*w)) + lambda*w\n",
    "    g = -yi*np.array(xi)/(1+np.exp(yi*np.matmul(np.transpose(xi),w))) + lam*np.array(w)\n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    '''\n",
    "    n = len(x/2): n samples will be half of the total samples\n",
    "    '''\n",
    "    objvals = []\n",
    "    for i in range(max_epoch):\n",
    "        combined = np.c_[y,x]\n",
    "        combined = np.random.permutation(combined)\n",
    "        x = combined[:, 1:]\n",
    "        y = combined[:, 0]\n",
    "        for j in range(len(x/2)):\n",
    "            obj, g = stochastic_objective_gradient(w, x[j],y[j],lam)\n",
    "            w = w - learning_rate*g\n",
    "        objvals.append(obj)\n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[1,2],\n",
    "#      [3,4],\n",
    "#      [5,6]]\n",
    "# b = [3, 5, 7]\n",
    "# combined =np.c_[b,a]\n",
    "# print(combined[:,1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.28008819 -0.60875369 -0.27189672 -0.50232686 -0.17908412  0.65644216\n",
      " -0.9135732  -1.08354344 -0.41110032  0.17394205 -1.91946926  0.04260872\n",
      " -1.315624   -1.43719378  0.35740504  1.01253442  0.10283325 -0.20247031\n",
      "  0.16960693  1.14851528 -1.14342389 -1.3097148  -0.95756804 -1.2641757\n",
      " -0.70311933 -0.00949126 -1.15686392 -1.31854769 -0.69681177 -0.79192927]\n",
      "\n",
      "Objective Values: [0.10611756880689428, 5.5697170551794854e-05, 0.20220240758509375, 0.06956300552528209, 0.526494413489385, 0.0039722267422143, 0.03048046997075291, 0.0002536822308764478, 0.0001385155972787995, 0.001498951116202685, 0.0035543002763128964, 0.009777392571634074, 0.0011754523152854386, 1.3077285049654823, 8.344418434508575e-06, 0.021555137728226103, 0.0036563337629635048, 0.001960995038201083, 2.3265189726100575e-08, 0.003074931867976262, 3.0830449304155487e-11, 0.02455659649418686, 0.003560165471092902, 0.0001499345858281011, 0.00046780519824584033, 0.00013356063279608776, 0.002325973125380265, 0.05457546846956387, 1.7572880437099556e-05, 0.00040641601324589423, 3.2279874276978013e-09, 0.0007329016003702932, 3.3804365830160886e-05, 0.010740815739616317, 0.21442420816680263, 0.001760639667639199, 0.08172158473874279, 0.00021256344464942793, 1.3840419159339207e-08, 8.43701389218478e-07, 0.007248739362871201, 0.1384570290473009, 0.0003852019252986978, 0.03254869831877392, 0.001782716396746637, 1.8541597337166806e-07, 0.021720682723800663, 0.019107356208520002, 0.006163601040491402, 5.116699294434071e-05, 0.001311121773611859, 0.04939385259012083, 0.0022252478063782676, 0.001438285865326459, 5.989106674627222e-06, 0.0001705010561574392, 0.57989939898058, 0.07450089093496916, 0.06445156908625685, 0.002659148787171339, 0.014426417808310938, 0.0001458938171287665, 0.2217516454426851, 1.9796947849392274e-06, 0.00032167322838793814, 0.001600859866645323, 0.0011297502525047516, 1.4674175207925185e-05, 0.01361934685197269, 0.10018084233486546, 2.050878714119452e-06, 0.0020327500429131655, 7.349676423018266e-14, 0.004138047686344176, 2.3966717496618624e-10, 1.1269590244133573e-05, 0.00816038608982352, 0.08964496140102853, 2.618992322003949e-07, 1.125670552963314e-05, 3.275315030829905e-06, 0.0008459679656123474, 2.4118112299749366e-05, 0.0003700149557505066, 1.93545666060757e-05, 0.007384876642802483, 3.219646771412902e-14, 0.0003557018592629474, 0.0017373915419954529, 0.06512890558717925, 0.0013243961926268112, 7.153137987179315e-05, 0.007152672029597247, 0.0002903145528473804, 0.06816389890591963, 0.14812855710623313, 8.809774854585691e-08, 0.048769957022306104, 1.3285696892991926e-05, 0.008950486236003094]\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w = [1/len(x_train[0])] * len(x_train[0])\n",
    "optWeightsS, objListS = sgd(x_train, y_train, 0, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsS)\n",
    "print()\n",
    "print(\"Objective Values:\", objListS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.31048119 -0.26991981 -0.30786508 -0.3342392  -0.13744879 -0.08968963\n",
      " -0.28956204 -0.36724129 -0.14262269  0.11668186 -0.41964762 -0.01743078\n",
      " -0.35815305 -0.35994156  0.02367386  0.11792003  0.08880901 -0.01109923\n",
      "  0.03602164  0.18455885 -0.41762642 -0.34778623 -0.39946561 -0.41955217\n",
      " -0.27320548 -0.18429349 -0.28013553 -0.37556196 -0.26249581 -0.14301862]\n",
      "\n",
      "Objective Values: [0.04107067483339894, 0.07000794531102825, 0.05492070954225427, 0.08420446762993383, 0.052695966577484814, 0.2136004198766184, 0.05380634855502666, 0.1625172604076046, 0.1360071637666369, 0.08487531841274785, 0.1099262212591025, 0.08632226438420681, 0.23926319771682109, 0.05568500532171883, 0.15654131990415557, 0.13223705225094984, 0.20936216955866774, 0.31905308519628384, 0.1756408188900033, 0.09750907679123981, 0.0997306499821273, 0.07931147344578061, 0.05966444503726757, 0.06286887472639036, 0.0979998401499306, 0.103351425900814, 0.16962800293974128, 0.13937320997683378, 0.7018866188588075, 0.11242224313209344, 0.1964266748589009, 0.05546031401671536, 0.05721524794849612, 0.07174577656362527, 0.11583704117324217, 0.15713218004887058, 1.7032779160810891, 0.07616770260957467, 0.1361347576058855, 0.05751973052868669, 0.05996506079819795, 0.6544509859924055, 0.06170326312388323, 0.48008495586456273, 0.06799518775235869, 0.055320570263865526, 0.20322274941418575, 0.07865597746863975, 0.3164004484523165, 0.07719433548213199, 0.2481086575462532, 0.1042445772201969, 0.14725093583676227, 0.0662714409681359, 0.055111451406393897, 0.11445532544875356, 0.18722769326812744, 0.05584440111288082, 0.07123352199567715, 0.3602562132865398, 0.07180852434206941, 0.05631390713320827, 0.21256794106068477, 0.05878403277305631, 0.06733168269406513, 0.11926650504828006, 0.05560332410986914, 0.07840195224268762, 0.1609559684491953, 0.0612362047839042, 0.0788781936323516, 0.06034095144979146, 0.05805931222679514, 0.06303378184612582, 0.12021060116120355, 0.07570696581092688, 0.11676861026709709, 0.4442029165681517, 0.24569436053357285, 0.0928847998900944, 0.09718614549658541, 0.06418269036042837, 0.054447320208368004, 0.44080190589361723, 0.38595197168890694, 0.15199033804122822, 0.2008324964456134, 0.08968427365181006, 0.15438449636054616, 0.07411450445357076, 0.15039030041237986, 0.16253759140898225, 0.062154122120755655, 0.20407570887305823, 0.06840136575671607, 0.11598540113928012, 0.06788808427303492, 0.06009805774016699, 0.10230714814410719, 0.05608989783994573]\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "optWeightsRS, objListRS = sgd(x_train, y_train, 0.05, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsRS)\n",
    "print()\n",
    "print(\"Objective Values:\", objListRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveMGBD(w, xi, yi, lam, b):\n",
    "    '''\n",
    "    I is a set of b indices: i.e. if b = 5, then I = [92,514,12,243,409] (randomly)\n",
    "    '''\n",
    "    total = 0;\n",
    "    for i in range(min(b,len(xi))):\n",
    "        total += np.log(1+np.exp(-yi[i]*np.matmul(np.transpose(xi[i]),w)))\n",
    "    total /= min(b,len(xi))\n",
    "    total += lam/2*(np.linalg.norm(w,2))**2\n",
    "    return total\n",
    "    \n",
    "def gradientMGBD(w, xi, yi, lam, b):\n",
    "    total = 0\n",
    "    for i in range(min(b,len(xi))):\n",
    "        total += -yi[i]*np.array(xi[i])/(1+np.exp(yi[i]*np.matmul(np.transpose(xi[i]),w)))\n",
    "    total /= min(b,len(xi))\n",
    "    total += lam*np.array(w)\n",
    "    return total\n",
    "\n",
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: b-by-1\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    # log(1+exp(-yi*(xi^T)*w)) + lambda/2*w**2\n",
    "    obj = objectiveMGBD(w, xi, yi, lam, 32)\n",
    "    # -yi*xi/(1+exp(yi*(xi^T)*w)) + lambda*w\n",
    "    g = gradientMGBD(w, xi, yi, lam, 32)\n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = 3, b = 2\n",
    "# w = [1,2,3]\n",
    "# xi = [[1,2,3],\n",
    "#      [4,5,6]]\n",
    "# yi = [-1, 1]\n",
    "# mb_objective_gradient(w, xi, yi, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    objvals = []\n",
    "    for i in range(max_epoch):\n",
    "        combined = np.c_[y,x]\n",
    "        combined = np.random.permutation(combined)\n",
    "        x = combined[:, 1:]\n",
    "        y = combined[:, 0]\n",
    "        for j in range(int(np.ceil(len(x)/32))):\n",
    "            obj, g = mb_objective_gradient(w, x[j*32:j*32+32],y[j*32:j*32+32],lam)\n",
    "            w = w - learning_rate*g\n",
    "        objvals.append(obj)\n",
    "    return w, objvals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.41921615 -0.37770103 -0.41752436 -0.4302057  -0.18172375 -0.17022533\n",
      " -0.34890826 -0.46108416 -0.16769478  0.17854879 -0.47899876 -0.03228133\n",
      " -0.41906205 -0.41231996  0.05077423  0.12203784  0.13749004 -0.01927628\n",
      "  0.05587232  0.2370661  -0.52431977 -0.45577138 -0.50638808 -0.50955525\n",
      " -0.3275469  -0.24669439 -0.32559715 -0.46157324 -0.30815235 -0.13036275]\n",
      "\n",
      "Objective Values: [0.5857850695687782, 0.37124385316782044, 0.433900897648733, 0.41492342422558426, 0.4600103890353083, 0.26502587997642685, 0.3581487449279173, 0.3324877575438352, 0.1927734394709257, 0.257676067258538, 0.243292549483371, 0.20131689396915048, 0.13934843785801282, 0.2471219134613191, 0.23593779573499715, 0.2719081713354759, 0.14542378901923556, 0.09930483440223156, 0.147499273492483, 0.16573105392809367, 0.31222320968095285, 0.19922791886396124, 0.15428372475897392, 0.18977633018053133, 0.1454162345603584, 0.17822684859233545, 0.13151328975257245, 0.16884693877517057, 0.1703035221466618, 0.12165874390162558, 0.18223147276702836, 0.0986984891582021, 0.28179756930903227, 0.14841260606694553, 0.1802263556883475, 0.19679921574616613, 0.2367490636999905, 0.0786463173590779, 0.3006826842250903, 0.08811754454509615, 0.06265669001983823, 0.22963690228498596, 0.021575473714676805, 0.10769869589534406, 0.1976203988831729, 0.04276746630503699, 0.029139324737729042, 0.07164906023118083, 0.41245787231230596, 0.04418022245749049, 0.0602005025469585, 0.1485939625035745, 0.06739635327753822, 0.10734433032871397, 0.13182220906147615, 0.16402953078460755, 0.12925222928320904, 0.1002995217344231, 0.016120445397520163, 0.05409800544824204, 0.11879736251639528, 0.07858966077169005, 0.04737433083870644, 0.02923218067544445, 0.13245317256738548, 0.11288251808497805, 0.07794472539843571, 0.09679418324464457, 0.12426616314093313, 0.0344758875957002, 0.03438392998276986, 0.040257141209691406, 0.098499417278082, 0.0884489333871741, 0.07239555763770075, 0.06095167909474896, 0.14622277461519492, 0.10766245449057803, 0.05820958068814846, 0.08928906261420136, 0.013426370127922676, 0.08218574387340181, 0.020111220435418017, 0.06483416997201068, 0.026669290975645243, 0.045766258731422815, 0.05388699391892982, 0.05120945589948552, 0.10393754123781282, 0.48594346052266385, 0.014414900087982557, 0.09797535186362263, 0.04888049373970387, 0.07871950560341383, 0.07323393717143516, 0.05565224641128857, 0.043077548858136916, 0.13514291801940784, 0.037433809777909256, 0.08235479238843268]\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w = [1/len(x_train[0])] * len(x_train[0])\n",
    "optWeightsM, objListM = mbgd(x_train, y_train, 0, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsM)\n",
    "print()\n",
    "print(\"Objective Values:\", objListM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.11630682 -0.07845609 -0.11703181 -0.11118783 -0.05290696 -0.07955286\n",
      " -0.09803795 -0.121291   -0.04791798  0.02515347 -0.08958762  0.0021318\n",
      " -0.08426952 -0.08244992  0.01938426 -0.01249275  0.00137047 -0.03641533\n",
      "  0.00817386  0.02477946 -0.127631   -0.09038099 -0.12665982 -0.11772238\n",
      " -0.07489575 -0.08841865 -0.0965638  -0.1271034  -0.07597892 -0.04298294]\n",
      "\n",
      "Objective Values: [0.6073739720271083, 0.5843377729431413, 0.3898815145023788, 0.6018429892653256, 0.3971372417778853, 0.3399612603774485, 0.3585326883249839, 0.3930125036072518, 0.5468621822918385, 0.5223885812618356, 0.46990666241648227, 0.2840593023174581, 0.3698505218334271, 0.3022674753015177, 0.372599486395349, 0.2793976272656892, 0.3682323404101513, 0.32919234510354867, 0.41768981719974096, 0.46188419730437147, 0.4958785601024286, 0.42553953186482263, 0.45883085691095365, 0.5206803573868137, 0.35147049665094243, 0.35276567911835083, 0.43094441287270585, 0.37934015099009727, 0.4000185632917451, 0.4081358339614647, 0.4584967397010603, 0.42895249107653305, 0.33305349410186014, 0.3889068316289461, 0.3771590812378407, 0.35596032482532036, 0.3768492902839513, 0.4859657191286392, 0.4236041297226131, 0.3887102659083258, 0.4718974467616128, 0.3061286066732748, 0.4996632744005296, 0.36752519871144695, 0.33800937374715007, 0.36065383697831566, 0.4537911966789533, 0.427785878997085, 0.5160078412790583, 0.34500977153810275, 0.46438174738852434, 0.4405446294739497, 0.41288266316870115, 0.5661725822784929, 0.4070045412989947, 0.39588207447695556, 0.3957501189675482, 0.48749681222548996, 0.4001808646517927, 0.3593611245064012, 0.5453118330418145, 0.3842443085666305, 0.49819696417429493, 0.4555809855002722, 0.38696451244734864, 0.4169857798784906, 0.526722424904031, 0.26924633284549887, 0.28095465508345757, 0.4895255522726033, 0.44417878416605777, 0.3553550346151225, 0.4899904288508516, 0.36415650137244293, 0.47086976251606694, 0.48114227142299226, 0.3815799603149422, 0.4387943704034081, 0.3344335866756699, 0.3128867415504035, 0.37980094767714123, 0.3792466550815222, 0.4967015616435076, 0.36824198592012314, 0.4962795039244311, 0.5220537872059141, 0.5307735161679505, 0.3989412103720157, 0.4093332818963729, 0.3966314844207348, 0.28010178905122374, 0.43599576657847394, 0.478145190417978, 0.456006635648276, 0.3702670419050551, 0.3163575001773036, 0.34962735310143456, 0.46668244696288624, 0.3988366360260889, 0.23945792426050264]\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w = [1/len(x_train[0])] * len(x_train[0])\n",
    "optWeightsRM, objListRM = mbgd(x_train, y_train, 1, 0.01, w)\n",
    "print(\"Optimal Weights:\", optWeightsRM)\n",
    "print()\n",
    "print(\"Objective Values:\", objListRM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f124587400>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(100),objListG,'r')\n",
    "plt.plot(range(100),objListS,'g')\n",
    "plt.plot(range(100),objListM,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    predictions = np.matmul(X,w)\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] < 0:\n",
    "            predictions[i] = -1\n",
    "        else:\n",
    "            predictions[i] = 1\n",
    "    return predictions\n",
    "def getAccuracy(predictions,labels):\n",
    "    count = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            count+=1\n",
    "    return count/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "\n",
      "Gradient Descent Accuracy: 0.9560439560439561\n",
      "Regularized Gradient Descent Accuracy: 0.9560439560439561\n",
      "\n",
      "Stochastic Gradient Descent Accuracy: 0.989010989010989\n",
      "Regularized Stochastic Gradient Descent Accuracy: 0.9824175824175824\n",
      "\n",
      "Mini-Batch Gradient Descent Accuracy: 0.9802197802197802\n",
      "Regularized Mini-Batch Gradient Descent Accuracy: 0.9604395604395605\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistric regression and regularized version\n",
    "print(\"TRAINING\")\n",
    "print()\n",
    "\n",
    "predG = predict(optWeightsG,x_train)\n",
    "accG = getAccuracy(predG,y_train)\n",
    "print(\"Gradient Descent Accuracy:\", accG)\n",
    "\n",
    "predRG = predict(optWeightsRG,x_train)\n",
    "accRG = getAccuracy(predRG,y_train)\n",
    "print(\"Regularized Gradient Descent Accuracy:\", accRG)\n",
    "\n",
    "print()\n",
    "\n",
    "predS = predict(optWeightsS,x_train)\n",
    "accS = getAccuracy(predS,y_train)\n",
    "print(\"Stochastic Gradient Descent Accuracy:\", accS)\n",
    "\n",
    "predRS = predict(optWeightsRS,x_train)\n",
    "accRS = getAccuracy(predRS,y_train)\n",
    "print(\"Regularized Stochastic Gradient Descent Accuracy:\", accRS)\n",
    "\n",
    "print()\n",
    "\n",
    "predM = predict(optWeightsM,x_train)\n",
    "accM = getAccuracy(predM,y_train)\n",
    "print(\"Mini-Batch Gradient Descent Accuracy:\", accM)\n",
    "\n",
    "predRM = predict(optWeightsRM,x_train)\n",
    "accRM = getAccuracy(predRM,y_train)\n",
    "print(\"Regularized Mini-Batch Gradient Descent Accuracy:\", accRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "\n",
      "Gradient Descent Accuracy: 0.9210526315789473\n",
      "Regularized Gradient Descent Accuracy: 0.9210526315789473\n",
      "\n",
      "Stochastic Gradient Descent Accuracy: 0.9649122807017544\n",
      "Regularized Stochastic Gradient Descent Accuracy: 0.956140350877193\n",
      "\n",
      "Mini-Batch Gradient Descent Accuracy: 0.956140350877193\n",
      "Regularized Mini-Batch Gradient Descent Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistric regression and regularized version\n",
    "print(\"TESTING\")\n",
    "print()\n",
    "\n",
    "predG = predict(optWeightsG,x_test)\n",
    "accG = getAccuracy(predG,y_test)\n",
    "print(\"Gradient Descent Accuracy:\", accG)\n",
    "\n",
    "predRG = predict(optWeightsRG,x_test)\n",
    "accRG = getAccuracy(predRG,y_test)\n",
    "print(\"Regularized Gradient Descent Accuracy:\", accRG)\n",
    "\n",
    "print()\n",
    "\n",
    "predS = predict(optWeightsS,x_test)\n",
    "accS = getAccuracy(predS,y_test)\n",
    "print(\"Stochastic Gradient Descent Accuracy:\", accS)\n",
    "\n",
    "predRS = predict(optWeightsRS,x_test)\n",
    "accRS = getAccuracy(predRS,y_test)\n",
    "print(\"Regularized Stochastic Gradient Descent Accuracy:\", accRS)\n",
    "\n",
    "print()\n",
    "\n",
    "predM = predict(optWeightsM,x_test)\n",
    "accM = getAccuracy(predM,y_test)\n",
    "print(\"Mini-Batch Gradient Descent Accuracy:\", accM)\n",
    "\n",
    "predRM = predict(optWeightsRM,x_test)\n",
    "accRM = getAccuracy(predRM,y_test)\n",
    "print(\"Regularized Mini-Batch Gradient Descent Accuracy:\", accRM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
